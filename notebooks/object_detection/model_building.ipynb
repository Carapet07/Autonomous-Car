{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# BDD100K Object Detection - Model Building\n",
    "\n",
    "This notebook builds and tests a simple YOLO model for object detection on the BDD100K dataset.\n",
    "\n",
    "## Overview\n",
    "- Load YOLO model\n",
    "- Train with minimal settings (1 epoch, 1 batch size)\n",
    "- Test predictions\n",
    "- Evaluate performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d83f7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Notebook directory: /Users/romacarapetean/Desktop/Projects/Autonomous-Car-main/notebooks/object_detection\n",
      "üìÅ Project directory: /Users/romacarapetean/Desktop/Projects/Autonomous-Car-main\n",
      "üìã Dataset Configuration:\n",
      "  path: /Users/romacarapetean/Desktop/Projects/Autonomous-Car-main/notebooks/object_detection/../../Datasets/ObjectDetectionSet/bdd100k/yolo_format\n",
      "  train: images/train\n",
      "  val: images/val\n",
      "  test: images/test\n",
      "  nc: 10\n",
      "  names: ['pedestrian', 'rider', 'car', 'truck', 'bus', 'train', 'motorcycle', 'bicycle', 'traffic light', 'traffic sign']\n",
      "\n",
      "üìã Classes (10): ['pedestrian', 'rider', 'car', 'truck', 'bus', 'train', 'motorcycle', 'bicycle', 'traffic light', 'traffic sign']\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Import YOLO\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Set up paths\n",
    "notebook_dir = Path.cwd()\n",
    "project_dir = notebook_dir.parent.parent\n",
    "sys.path.append(str(project_dir))\n",
    "\n",
    "print(f\"üìÅ Notebook directory: {notebook_dir}\")\n",
    "print(f\"üìÅ Project directory: {project_dir}\")\n",
    "\n",
    "# Dataset configuration\n",
    "dataset_dir = Path(\"../../Datasets/ObjectDetectionSet/bdd100k/yolo_format\")\n",
    "config_file = dataset_dir / \"data.yaml\"\n",
    "\n",
    "# Load dataset configuration\n",
    "with open(config_file, 'r') as f:\n",
    "    dataset_config = yaml.safe_load(f)\n",
    "\n",
    "print(\"üìã Dataset Configuration:\")\n",
    "for key, value in dataset_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Class information\n",
    "class_names = dataset_config['names']\n",
    "num_classes = dataset_config['nc']\n",
    "\n",
    "print(f\"\\nüìã Classes ({num_classes}): {class_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1244de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading pre-trained YOLO model...\n",
      "‚úÖ Loaded model from: /Users/romacarapetean/Desktop/Projects/Autonomous-Car-main/notebooks/traffic_signs_detection/yolov8n.pt\n",
      "üìä Model info:\n",
      "  Model type: <class 'ultralytics.models.yolo.model.YOLO'>\n",
      "  Total parameters: 3,157,200\n",
      "  Trainable parameters: 0\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained YOLO model\n",
    "print(\"üîÑ Loading pre-trained YOLO model...\")\n",
    "\n",
    "# Load pre-trained YOLO model (will download if not available)\n",
    "model = YOLO('yolov8n.pt')\n",
    "print(\"‚úÖ Loaded pre-trained YOLO model from ultralytics\")\n",
    "\n",
    "print(f\"üìä Model info:\")\n",
    "print(f\"  Model type: {type(model)}\")\n",
    "\n",
    "# Check model parameters\n",
    "total_params = sum(p.numel() for p in model.model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c24dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Training Configuration:\n",
      "  data: ../../Datasets/ObjectDetectionSet/bdd100k/yolo_format/data.yaml\n",
      "  epochs: 1\n",
      "  batch: 1\n",
      "  imgsz: 640\n",
      "  device: cpu\n",
      "  workers: 0\n",
      "  project: ../../models/object_detection\n",
      "  name: bdd100k_test\n",
      "  exist_ok: True\n",
      "  pretrained: True\n",
      "  optimizer: auto\n",
      "  verbose: True\n",
      "  seed: 42\n",
      "  deterministic: True\n",
      "  single_cls: False\n",
      "  rect: False\n",
      "  cos_lr: False\n",
      "  close_mosaic: 0\n",
      "  resume: False\n",
      "  amp: False\n",
      "  fraction: 0.1\n",
      "  cache: False\n",
      "  val: True\n",
      "  plots: True\n",
      "  save: True\n",
      "  save_period: -1\n",
      "\n",
      "‚ö†Ô∏è  Note: This is a minimal training run for testing purposes!\n",
      "   - Only 1 epoch\n",
      "   - Batch size of 1\n",
      "   - Using only 10% of data\n",
      "   - CPU training (slow but safe for testing)\n"
     ]
    }
   ],
   "source": [
    "# Training configuration for testing\n",
    "training_config = {\n",
    "    'data': str(config_file),           # Dataset configuration file\n",
    "    'epochs': 1,                        # Number of epochs (minimal for testing)\n",
    "    'batch': 1,                         # Batch size (minimal for testing)\n",
    "    'imgsz': 640,                       # Image size\n",
    "    'device': 'cpu',                    # Use CPU for testing\n",
    "    'workers': 0,                       # No multiprocessing for testing\n",
    "    'project': '../../models/object_detection',  # Output directory\n",
    "    'name': 'bdd100k_test',             # Experiment name\n",
    "    'exist_ok': True,                   # Overwrite existing experiment\n",
    "    'pretrained': True,                 # Use pre-trained weights\n",
    "    'optimizer': 'auto',                # Auto-select optimizer\n",
    "    'verbose': True,                    # Verbose output\n",
    "    'seed': 42,                         # Random seed\n",
    "    'deterministic': True,              # Deterministic training\n",
    "    'single_cls': False,                # Multi-class detection\n",
    "    'rect': False,                      # Rectangular training\n",
    "    'cos_lr': False,                    # Cosine learning rate\n",
    "    'close_mosaic': 0,                  # Disable mosaic augmentation\n",
    "    'resume': False,                    # Don't resume from checkpoint\n",
    "    'amp': False,                       # Disable mixed precision\n",
    "    'fraction': 0.1,                    # Use only 10% of data for testing\n",
    "    'cache': False,                     # Don't cache images\n",
    "    'val': True,                        # Run validation\n",
    "    'plots': True,                      # Generate plots\n",
    "    'save': True,                       # Save results\n",
    "    'save_period': -1,                  # Save only at end\n",
    "}\n",
    "\n",
    "print(\"üìã Training Configuration:\")\n",
    "for key, value in training_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Note: This is a minimal training run for testing purposes!\")\n",
    "print(f\"   - Only 1 epoch\")\n",
    "print(f\"   - Batch size of 1\")\n",
    "print(f\"   - Using only 10% of data\")\n",
    "print(f\"   - CPU training (slow but safe for testing)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92f68ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\n",
      "‚è±Ô∏è  This may take a while even with minimal settings...\n",
      "New https://pypi.org/project/ultralytics/8.3.174 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.170 üöÄ Python-3.13.1 torch-2.7.1 CPU (Apple M2)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=1, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=0, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=../../Datasets/ObjectDetectionSet/bdd100k/yolo_format/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=1, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=0.1, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=/Users/romacarapetean/Desktop/Projects/Autonomous-Car-main/notebooks/traffic_signs_detection/yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=bdd100k_test, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=../../models/object_detection, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=../../models/object_detection/bdd100k_test, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=0, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753262  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,012,798 parameters, 3,012,782 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 213.3¬±58.6 MB/s, size: 58.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/romacarapetean/Desktop/Projects/Autonomous-Car-main/Datasets/ObjectDetectionSet/bdd100k/yolo_format/labels/train... 115 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:00<00:00, 3431.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/romacarapetean/Desktop/Projects/Autonomous-Car-main/Datasets/ObjectDetectionSet/bdd100k/yolo_format/labels/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 179.1¬±58.3 MB/s, size: 51.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/romacarapetean/Desktop/Projects/Autonomous-Car-main/Datasets/ObjectDetectionSet/bdd100k/yolo_format/labels/val... 10000 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:02<00:00, 3724.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/romacarapetean/Desktop/Projects/Autonomous-Car-main/Datasets/ObjectDetectionSet/bdd100k/yolo_format/labels/val.cache\n",
      "Plotting labels to ../../models/object_detection/bdd100k_test/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m../../models/object_detection/bdd100k_test\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1         0G      1.852      4.152      1.273         48        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:20<00:00,  5.52it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [08:16<00:00, 10.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      10000     185074     0.0136      0.138     0.0264     0.0174\n",
      "\n",
      "1 epochs completed in 0.144 hours.\n",
      "Optimizer stripped from ../../models/object_detection/bdd100k_test/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from ../../models/object_detection/bdd100k_test/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating ../../models/object_detection/bdd100k_test/weights/best.pt...\n",
      "Ultralytics 8.3.170 üöÄ Python-3.13.1 torch-2.7.1 CPU (Apple M2)\n",
      "Model summary (fused): 72 layers, 3,007,598 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [07:41<00:00, 10.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      10000     185074     0.0135      0.139     0.0264     0.0174\n",
      "            pedestrian       3220      13262    0.00325     0.0112    0.00165   0.000943\n",
      "                 rider        515        649    0.00017      0.133   0.000151   4.66e-05\n",
      "                   car       9879     102506      0.112      0.322       0.22      0.146\n",
      "                 truck       2689       4245    0.00289      0.332    0.00975    0.00569\n",
      "                   bus       1242       1597    0.00269      0.292    0.00526    0.00362\n",
      "                 train         14         15   5.92e-06     0.0667   3.94e-06   2.36e-06\n",
      "               bicycle        578       1007   0.000418     0.0914   0.000321   0.000123\n",
      "         traffic light       5653      26885    3.2e-05   0.000186   1.63e-05   4.27e-06\n",
      "          traffic sign       8221      34908   0.000401    0.00358   0.000213   9.41e-05\n",
      "Speed: 0.3ms preprocess, 36.9ms inference, 0.0ms loss, 4.3ms postprocess per image\n",
      "Results saved to \u001b[1m../../models/object_detection/bdd100k_test\u001b[0m\n",
      "‚úÖ Training completed in 1002.69 seconds\n",
      "\n",
      "üìä Training Results:\n",
      "  metrics/precision(B): 0.013518422954405994\n",
      "  metrics/recall(B): 0.1389831758232544\n",
      "  metrics/mAP50(B): 0.026423190167756436\n",
      "  metrics/mAP50-95(B): 0.017408402145304678\n",
      "  fitness: 0.018309880947549855\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"‚è±Ô∏è  This may take a while even with minimal settings...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    results = model.train(**training_config)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"‚úÖ Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Print training results\n",
    "    print(f\"\\nüìä Training Results:\")\n",
    "    if hasattr(results, 'results_dict'):\n",
    "        for key, value in results.results_dict.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {str(e)}\")\n",
    "    print(f\"üí° This might be due to dataset issues or insufficient resources.\")\n",
    "    print(f\"üí° Check the dataset configuration and try again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af23e0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing model on sample images...\n",
      "üì∏ Testing on: becfc708-eaf36a96.jpg\n",
      "üîç Making prediction...\n",
      "\n",
      "image 1/1 /Users/romacarapetean/Desktop/Projects/Autonomous-Car-main/notebooks/object_detection/../../Datasets/ObjectDetectionSet/bdd100k/yolo_format/images/val/becfc708-eaf36a96.jpg: 384x640 (no detections), 45.4ms\n",
      "Speed: 1.4ms preprocess, 45.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "‚ö†Ô∏è  No objects detected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ Model building test completed!\n",
      "üìÅ Check the models directory for results and trained weights.\n"
     ]
    }
   ],
   "source": [
    "# Test on sample images\n",
    "print(\"üîç Testing model on sample images...\")\n",
    "\n",
    "test_images_dir = dataset_dir / \"images/val\"\n",
    "if test_images_dir.exists():\n",
    "    test_images = list(test_images_dir.glob('*.jpg'))\n",
    "    \n",
    "    if test_images:\n",
    "        # Select random sample\n",
    "        sample_image = random.choice(test_images)\n",
    "        print(f\"üì∏ Testing on: {sample_image.name}\")\n",
    "        \n",
    "        # Make prediction\n",
    "        print(\"üîç Making prediction...\")\n",
    "        predictions = model(str(sample_image), conf=0.25, iou=0.45)\n",
    "        \n",
    "        if predictions:\n",
    "            # Load and display image\n",
    "            img = Image.open(sample_image)\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "            ax.imshow(img)\n",
    "            \n",
    "            # Draw predictions if any\n",
    "            pred = predictions[0]\n",
    "            if pred.boxes is not None and len(pred.boxes) > 0:\n",
    "                for i in range(len(pred.boxes)):\n",
    "                    # Get bounding box coordinates\n",
    "                    x1, y1, x2, y2 = pred.boxes.xyxy[i].cpu().numpy()\n",
    "                    confidence = pred.boxes.conf[i].cpu().numpy()\n",
    "                    class_id = int(pred.boxes.cls[i].cpu().numpy())\n",
    "                    \n",
    "                    # Get class name\n",
    "                    if class_id < len(class_names):\n",
    "                        class_name = class_names[class_id]\n",
    "                    else:\n",
    "                        class_name = f\"class_{class_id}\"\n",
    "                    \n",
    "                    # Create rectangle\n",
    "                    rect = patches.Rectangle(\n",
    "                        (x1, y1), x2-x1, y2-y1, \n",
    "                        linewidth=2, \n",
    "                        edgecolor='red', \n",
    "                        facecolor='none'\n",
    "                    )\n",
    "                    ax.add_patch(rect)\n",
    "                    \n",
    "                    # Add label\n",
    "                    label = f\"{class_name} {confidence:.2f}\"\n",
    "                    ax.text(x1, y1-5, label, \n",
    "                            color='red', \n",
    "                            fontsize=10, fontweight='bold',\n",
    "                            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.8))\n",
    "                \n",
    "                print(f\"‚úÖ Detected {len(pred.boxes)} objects\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  No objects detected\")\n",
    "            \n",
    "            ax.set_title(f\"Model Prediction - {sample_image.name}\", fontsize=14, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No predictions returned\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  No test images found in {test_images_dir}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Test images directory not found: {test_images_dir}\")\n",
    "\n",
    "print(\"\\nüéâ Model building test completed!\")\n",
    "print(\"üìÅ Check the models directory for results and trained weights.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78171d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluating model performance...\n",
      "WARNING ‚ö†Ô∏è 'save_hybrid' is deprecated and will be removed in in the future.\n",
      "Ultralytics 8.3.170 üöÄ Python-3.13.1 torch-2.7.1 CPU (Apple M2)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 176.2¬±64.6 MB/s, size: 57.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/romacarapetean/Desktop/Projects/Autonomous-Car-main/Datasets/ObjectDetectionSet/bdd100k/yolo_format/labels/val.cache... 10000 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [06:30<00:00, 25.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      10000     185074     0.0142      0.141     0.0274     0.0178\n",
      "            pedestrian       3220      13262    0.00305     0.0123    0.00156   0.000829\n",
      "                 rider        515        649   0.000175      0.131   0.000156   4.64e-05\n",
      "                   car       9879     102506      0.118      0.325      0.229       0.15\n",
      "                 truck       2689       4245    0.00318      0.341     0.0102    0.00583\n",
      "                   bus       1242       1597    0.00262      0.302    0.00545    0.00364\n",
      "                 train         14         15   5.52e-06     0.0667   3.88e-06   2.33e-06\n",
      "               bicycle        578       1007   0.000358     0.0914   0.000294   0.000106\n",
      "         traffic light       5653      26885   4.31e-05   0.000298   2.18e-05   6.29e-06\n",
      "          traffic sign       8221      34908   0.000388    0.00381   0.000207   8.57e-05\n",
      "Speed: 0.2ms preprocess, 29.5ms inference, 0.0ms loss, 3.2ms postprocess per image\n",
      "Saving ../../models/object_detection/bdd100k_test_val/predictions.json...\n",
      "Results saved to \u001b[1m../../models/object_detection/bdd100k_test_val\u001b[0m\n",
      "‚úÖ Validation completed!\n",
      "\n",
      "üìä Validation Metrics:\n",
      "  metrics/precision(B): 0.0142\n",
      "  metrics/recall(B): 0.1414\n",
      "  metrics/mAP50(B): 0.0274\n",
      "  metrics/mAP50-95(B): 0.0178\n",
      "  fitness: 0.0188\n",
      "\n",
      "üìä Key Performance Metrics:\n",
      "  mAP@0.5: 0.0274\n",
      "  mAP@0.5:0.95: 0.0178\n",
      "\n",
      "üìä Per-class mAP@0.5:\n",
      "  pedestrian: 0.0008\n",
      "  rider: 0.0000\n",
      "  car: 0.1497\n",
      "  truck: 0.0058\n",
      "  bus: 0.0036\n",
      "  train: 0.0000\n",
      "  motorcycle: 0.0178\n",
      "  bicycle: 0.0001\n",
      "  traffic light: 0.0000\n",
      "  traffic sign: 0.0001\n",
      "\n",
      "üèÅ Model building and testing completed!\n",
      "üìù Summary:\n",
      "  - Loaded pre-trained YOLO model\n",
      "  - Configured for BDD100K dataset (10 classes)\n",
      "  - Ran minimal training (1 epoch, 1 batch)\n",
      "  - Tested on sample images\n",
      "  - Evaluated performance metrics\n",
      "\n",
      "üí° For full training, use the training script:\n",
      "   python scripts/object_detection/train_model.py --data ../../Datasets/ObjectDetectionSet/bdd100k/yolo_format/data.yaml\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance (if training was successful)\n",
    "try:\n",
    "    # Validate on a subset of data\n",
    "    print(\"üìä Evaluating model performance...\")\n",
    "    \n",
    "    val_results = model.val(\n",
    "        data=str(config_file),\n",
    "        device='cpu',\n",
    "        verbose=True,\n",
    "        save_json=True,\n",
    "        save_hybrid=False,\n",
    "        conf=0.001,\n",
    "        iou=0.6,\n",
    "        max_det=300,\n",
    "        half=False,\n",
    "        batch=1,\n",
    "        project='../../models/object_detection',\n",
    "        name='bdd100k_test_val'\n",
    "    )\n",
    "    \n",
    "    if val_results:\n",
    "        print(f\"‚úÖ Validation completed!\")\n",
    "        \n",
    "        # Print key metrics\n",
    "        metrics = val_results.results_dict\n",
    "        if metrics:\n",
    "            print(f\"\\nüìä Validation Metrics:\")\n",
    "            for key, value in metrics.items():\n",
    "                if isinstance(value, (int, float)):\n",
    "                    print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "        # Get mAP scores\n",
    "        if hasattr(val_results, 'box'):\n",
    "            if hasattr(val_results.box, 'map'):\n",
    "                print(f\"\\nüìä Key Performance Metrics:\")\n",
    "                print(f\"  mAP@0.5: {val_results.box.map50:.4f}\")\n",
    "                print(f\"  mAP@0.5:0.95: {val_results.box.map:.4f}\")\n",
    "                \n",
    "                # Per-class mAP\n",
    "                if hasattr(val_results.box, 'maps'):\n",
    "                    print(f\"\\nüìä Per-class mAP@0.5:\")\n",
    "                    for i, class_map in enumerate(val_results.box.maps):\n",
    "                        if i < len(class_names):\n",
    "                            print(f\"  {class_names[i]}: {class_map:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Validation failed: {str(e)}\")\n",
    "    print(f\"üí° This might be due to insufficient training or data issues.\")\n",
    "\n",
    "print(f\"\\nüèÅ Model building and testing completed!\")\n",
    "print(f\"üìù Summary:\")\n",
    "print(f\"  - Loaded pre-trained YOLO model\")\n",
    "print(f\"  - Configured for BDD100K dataset ({num_classes} classes)\")\n",
    "print(f\"  - Ran minimal training (1 epoch, 1 batch)\")\n",
    "print(f\"  - Tested on sample images\")\n",
    "print(f\"  - Evaluated performance metrics\")\n",
    "print(f\"\\nüí° For full training, use the training script:\")\n",
    "print(f\"   python scripts/object_detection/train_model.py --data Datasets/ObjectDetectionSet/bdd100k/yolo_format/data.yaml\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
